{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Fairness in Speaker Verification \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Speaker Verification\n",
    "\n",
    "### What is Speaker Verification?\n",
    "\n",
    "Speaker verification is a form of biometric authentication that determines whether a given speech sample matches a claimed identity. This technology is widely used in applications like voice-based device unlocking, personalized virtual assistants, and security systems. The core idea is to extract unique features from a person's voice ‚Äî much like a fingerprint ‚Äî and use them to verify their identity. The process typically involves converting speech into a numerical representation called an embedding and then comparing this embedding to previously stored ones. If the similarity score exceeds a certain threshold, the system confirms the identity; otherwise, it rejects it. This makes speaker verification powerful but also sensitive to variations in audio data and speaker characteristics.\n",
    "\n",
    "### Datasets for Speaker Verification\n",
    "\n",
    "    VoxCeleb: One of the most popular datasets for speaker verification is VoxCeleb. It is a widely-used dataset with speech samples from thousands of speakers across different nationalities and genders. Metadata includes speaker ID, gender, and nationality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore VoxCeleb Metadata\n",
    "\n",
    "Let's start by loading and exploring the VoxCeleb metadata to get a sense of the distribution of speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metadata (example CSV)\n",
    "metadata_path = \"/home/santhwanat1029@alabsad.fau.de/data-governance-seminar/data-governance-seminar/dataHDD/voxceleb/voxceleb_trainer/data/vox1_meta.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Inspect data\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù **Exercise 1 :**   Explore Gender and Nationality Distribution\n",
    "\n",
    "In this exercise, you will visualize the distribution of speakers based on gender and nationality. This will help you understand the dataset composition and identify potential representation biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot gender distribution\n",
    "metadata['gender'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Gender Distribution in VoxCeleb\")\n",
    "plt.show()\n",
    "\n",
    "# Plot nationality distribution\n",
    "metadata['nationality'].value_counts().head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Nationalities in VoxCeleb\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Understanding Data Biases\n",
    "\n",
    "Bias can inadvertently creep into machine learning systems, including speaker verification models. Biases often arise from imbalances or limitations in the training data, which can cause models to perform poorly for certain groups of people. Let‚Äôs break this down into different types of bias to understand how they manifest in audio data.\n",
    "Types of Bias:\n",
    "\n",
    "    Historical Bias: This occurs when societal inequalities are reflected in the dataset itself. For instance, if VoxCeleb contains more male speakers than female speakers, the model may become biased toward male voices, leading to higher error rates for women.\n",
    "\n",
    "    Representation Bias: When some groups are underrepresented in the dataset, the model may struggle to generalize to those groups. For example, if most speakers in the dataset are from a handful of nationalities, the system may perform poorly for speakers with less-represented accents.\n",
    "\n",
    "    Measurement Bias:  Measurement bias arises when the way data is collected or labeled distorts reality. In speaker verification, gender labels can be misleading ‚Äî for example, a high-pitched male voice might be misclassified as female, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù **Exercise 2 :**  Analyze Bias in VoxCeleb\n",
    "\n",
    "You can explore potential biases by looking at the distribution of gender and speech durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation Bias \n",
    "\n",
    "gender_counts = metadata['gender'].value_counts(normalize=True)\n",
    "print(\"Gender Representation:\\n\", gender_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the historical bias in detail, perform the following task . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù **Task 1 :** Analyze Gender and Nationality Distribution\n",
    "\n",
    "In this task, you‚Äôll compute the percentage distribution for gender and nationality, and explore their intersections. This will help you identify potential imbalances in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gender distribution percentages\n",
    "gender_counts = metadata['gender'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Compute nationality distribution percentages\n",
    "nationality_counts = metadata['nationality'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Compute intersection of gender and nationality\n",
    "intersection_counts = metadata.groupby(['gender', 'nationality']).size()\n",
    "intersection_percentages = (intersection_counts / len(metadata)) * 100\n",
    "\n",
    "# Display results\n",
    "print(\"Gender Distribution (%):\")\n",
    "print(gender_counts)\n",
    "\n",
    "print(\"\\nTop 10 Nationality Distribution (%):\")\n",
    "print(nationality_counts.head(10))\n",
    "\n",
    "print(\"\\nIntersection of Gender and Nationality (%):\")\n",
    "print(intersection_percentages.sort_values(ascending=False).head(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ **Your Goal:**\n",
    "- **Understand Gender Distribution:** See how speakers are distributed across gender categories.\n",
    "- **Examine Nationality Representation:** Check which nationalities dominate the dataset.\n",
    "- **Explore Intersectional Bias:** Discover which gender-nationality combinations are most or least represented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, reflect on the following:\n",
    "- Are certain gender or nationality groups overrepresented or underrepresented?\n",
    "- How might these imbalances affect the performance of a speaker verification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù **Task 2 :**\n",
    "\n",
    "Now listen to the following audio file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Provide the full path to your audio file\n",
    "audio_file_path = '/home/santhwanat1029@alabsad.fau.de/data-governance-seminar/data-governance-seminar/dataHDD/voxceleb/voxceleb_trainer/data/voxceleb1/id10035/9VR7wckrdP8/00003.wav'\n",
    "\n",
    "# Embed the audio file using HTML\n",
    "audio_html = f\"\"\"\n",
    "    <audio controls>\n",
    "        <source src=\"file://{audio_file_path}\" type=\"audio/wav\">\n",
    "        Your browser does not support the audio element.\n",
    "    </audio>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(audio_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again listen to the following audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Provide the full path to your audio file\n",
    "audio_file_path = '/home/santhwanat1029@alabsad.fau.de/data-governance-seminar/data-governance-seminar/dataHDD/voxceleb/voxceleb_trainer/data/voxceleb1/id10066/1Kr6tGO56H8/00001.wav'\n",
    "\n",
    "# Embed the audio file using HTML\n",
    "audio_html = f\"\"\"\n",
    "    <audio controls>\n",
    "        <source src=\"file://{audio_file_path}\" type=\"audio/wav\">\n",
    "        Your browser does not support the audio element.\n",
    "    </audio>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(audio_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 : Are binary gender labels truly useful for speaker verification, given the natural variation in pitch and voice characteristics? Give reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Speaker Verification Model\n",
    "\n",
    "To experiment with speaker verification, we can train a model based on a ResNet architecture. ResNet models are commonly used for speaker verification tasks due to their ability to learn rich audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using a ResNet-based Model\n",
    "import torch\n",
    "from models import ResNetSpeakerModel  # Hypothetical model\n",
    "\n",
    "# Initialize model\n",
    "model = ResNetSpeakerModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we need to evaluate how well it distinguishes between speakers and whether its performance varies across different demographic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### False Positives and False Negatives  \n",
    "\n",
    "\n",
    "False Positive (FP): This occurs when the model incorrectly predicts a positive class for an instance that actually belongs to the negative class.\n",
    "\n",
    "For example, imagine a disease detection model. If the model predicts that a healthy person has the disease, it would be a False Positive.\n",
    "\n",
    "False Negative (FN): This happens when the model incorrectly predicts a negative class for an instance that actually belongs to the positive class.\n",
    "\n",
    "For the disease detection example, if the model predicts that a sick person is healthy, it would be a False Negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Threshold\n",
    "\n",
    "The threshold is a critical component of speaker verification. It determines the score above which a speaker is considered verified. Choosing the right threshold is a trade-off: lowering it reduces false negatives but increases false positives, and vice versa. The threshold can also have unequal effects on different groups, contributing to fairness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Threshold Affects FP and FN\n",
    "\n",
    "When we are using a model that outputs probabilities for classes (for example, the probability that a patient has a disease), we can set a threshold to decide the cutoff point for classification:\n",
    "\n",
    "    If the model‚Äôs probability output for an instance is above the threshold, we classify it as positive (e.g., the person has the disease).\n",
    "    If the model‚Äôs probability is below the threshold, we classify it as negative (e.g., the person does not have the disease).\n",
    "\n",
    "As the threshold increases or decreases, the number of False Positives and False Negatives will change:\n",
    "\n",
    "    Lower threshold (closer to 0): More instances will be classified as positive, so False Positives (FP) may increase, and False Negatives (FN) may decrease.\n",
    "    Higher threshold (closer to 1): More instances will be classified as negative, so False Positives (FP) will decrease, and False Negatives (FN) may increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Effect of Thresholds on FP and FN\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "# Generate example data: 100 samples with random probabilities for class 1\n",
    "np.random.seed(42)\n",
    "y_true = np.random.choice([0, 1], size=100)  # True labels (0 = Negative, 1 = Positive)\n",
    "y_prob = np.random.rand(100)  # Predicted probabilities for class 1\n",
    "\n",
    "# Function to compute FP and FN given a threshold\n",
    "def calculate_fp_fn(threshold):\n",
    "    # Predicted labels based on the threshold\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate False Positives and False Negatives\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))  # Predicted positive, actual negative\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))  # Predicted negative, actual positive\n",
    "    \n",
    "    return FP, FN\n",
    "\n",
    "# Function to update the plot and display FP/FN based on threshold\n",
    "def update_plot(threshold):\n",
    "    FP, FN = calculate_fp_fn(threshold)\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['False Positives', 'False Negatives'], [FP, FN], color=['red', 'blue'])\n",
    "    plt.title(f'FP and FN at Threshold {threshold:.2f}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Display FP and FN\n",
    "    print(f'False Positives (FP): {FP}')\n",
    "    print(f'False Negatives (FN): {FN}')\n",
    "\n",
    "# Create an interactive widget to adjust threshold\n",
    "interactive_plot = interactive(update_plot, threshold=(0.0, 1.0, 0.01))\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Model Performance\n",
    "\n",
    "Once the model is trained, it outputs similarity scores for speaker pairs. We can use these scores to visualize the system's performance through Detection Error Tradeoff (DET) curves, which show the balance between false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scores\n",
    "scores_df = pd.read_csv(\"./scores.csv\")\n",
    "\n",
    "# Plot DET Curve\n",
    "from sklearn.metrics import det_curve\n",
    "fpr, fnr, thresholds = det_curve(scores_df['true_label'], scores_df['score'])\n",
    "\n",
    "plt.plot(fpr, fnr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('False Negative Rate')\n",
    "plt.title('DET Curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
